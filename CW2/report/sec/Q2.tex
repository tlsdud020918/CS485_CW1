\section{RF classifier}
\label{sec:intro_q2}

The random forest model has several key hyperparameters, so we fixed the optimal parameters in the sequence described below. Also, to ensure result stability, each test was executed 10 times. The average of 10 results displayed in a graph, and all the confusion matrix represents the best result among those. All training and testing time was measured excluding the codebook generation and vector quantization time; in other words, only the time spent on random forest classification applied on the 256-dimension quantized vectors was measured.
\begin{enumerate}
	\item Number of Trees \& Tree Depth: We tested using an axis-aligned weak learner with the split number fixed at 10.
	\item Split Number (Randomness Parameter): Optimal number of trees\&tree depth derived in step 1 and axis-aligned weak learner.
	\item Type of Weak Learner: Optimal values for other parameters determined in steps 1 and 2.
\end{enumerate}

%-------------------------------------------------------------------------
\subsection{Number of trees \& The depth of trees}
\label{subsec:Q2_1}
Changing the number of trees and depth, we extracted the result in \cref{fig:q2-fig1}. In the graph, the best accuracy 0.685 occurred at $N \text{(number of trees)} = 250$ and $D\text{ (depth)}=8$, explained as follows:
\begin{itemize}
	\item Number of Trees: A single tree in random forests too weak for classification; therefore, we can improve model through ensembling. As shown in \cref{fig:q2-fig2}, the test accuracy converges near $N=250$, which is selected as optimal parameter.
	\item Tree Depth: As shown in \cref{fig:q2-fig3}, for a given $N$, test accuracy initially increases with tree depth but later decreases due to overfitting. Notably, the optimal depth $D$ increases with $N$, indicating that larger $N$ usually result in better generalization.
\end{itemize}

\noindent
Given $N$ (number of trees), $D$ (tree depth), and $\rho$ (number of split attempts per node), the theoretical complexity is derived as follows. Our training time results closely align with these predictions, as shown in \cref{fig:q2-fig2} and \cref{fig:q2-fig3}. However, testing time—typically over 10 times shorter than training time—is more sensitive to noise from memory loading. Contrary to expectations, the testing time graph with respect to $D$ does not seems like linear complexity. This deviation might arise from noise introduced by the exponentially increasing memory usage as tree depth grows.

\begin{itemize}
	\item Training time: $O(ND\rho \times \text{number of training data})$
	\item Testing time: $O(ND \times \text{number of testing data})$
	\item Space complexity (= Number of nodes): $O(2^{D} \times N)$
\end{itemize}

\begin{figure}
	\centering
	\begin{subfigure}[H]{\linewidth}
		\centering
		\includegraphics[width=\linewidth]{image/q2-fig1-1.png}
		\caption{Training\&Testing accuracy and time according to the number of tree and the depth of tree}
		\label{fig:q2-fig1}
	\end{subfigure}
	\begin{subfigure}[H]{\linewidth}
		\centering
		\includegraphics[width=0.8\linewidth]{image/q2-fig2.png}
		\caption{Train\&Test accuracy and time according to the number of tree (D = 8)}
		\label{fig:q2-fig2}
	\end{subfigure}
	\begin{subfigure}[H]{\linewidth}
		\centering
		\includegraphics[width=0.8\linewidth]{image/q2-fig3.png}
		\caption{Training\&Testing accuracy and time according to the depth of tree (N = 250)}
		\label{fig:q2-fig3}
	\end{subfigure}
	\caption{Train\&Test accuracy and time according to the number and depth of trees}
\end{figure}
\noindent
Moreover, our code do not utilize parallelization of each tree's growth. Based on theoretical insights from course materials, training each tree in parallel could reduce the time complexity from $O(N)$ to $O(1)$ with respect to the number of trees.

\subsection{Randomness parameter}
We randomly select dimensions and threshold values to create the split function. For each split function, we attempt $\rho$ random splits and choose the one with highest information gain. There is a trade-off in the magnitude of $\rho$ value: If the $\rho$ value is too low, fewer splits are attempted, reducing similarity between trees but increasing the risk of less-optimal split. Conversely, as $\rho$ increases, more split functions are tested, leading to greater similarity among trees, which decreases the advantage of ensemble multiple trees. As shown in \cref{fig:q2-fig4}, test accuracy initially increases with higher $\rho$ values until $\rho=10$ and then decreases, as expected. Training and testing time is same as the expectation in \cref{subsec:Q2_1}.

\begin{figure}
	\centering
	\includegraphics[width=0.55\linewidth]{image/q2-fig4.png}
	\caption{Training\&Testing accuracy and time according to the randomness parameter}
	\label{fig:q2-fig4}
\end{figure}

\subsection{Impact of the vocabulary size on classification accuracy}
We varied the K-means vocabulary size while keeping the random forest classification parameters fixed. As shown in \cref{fig:q2-fig5}, the vocabulary size in vector quantization significantly impacts both training and testing accuracy. For small $K$, insufficient feature extraction leads to underfitting, resulting in low accuracy. Both training and testing accuracy improves rapidly up to $K=16$, but the rate of increase diminishes beyond this point. Notably, as $K$ increases from 256 to 512, training accuracy slightly improves, but testing accuracy declines. This might occur because vector quantization begins to capture unnecessary small details, reducing inner-class vector similarity.

\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{image/q2-fig5.png}
	\caption{Training\&Testing accuracy according to the K-means vocabulary size}
	\label{fig:q2-fig5}
\end{figure}

\subsection{Impact of the different types of weak-learners}
\cref{fig:q2-fig6} shows the impact of weak learner. As expected, the axis-aligned method provides sufficiently good testing accuracy with shorter training and testing times. However, the two-pixel test achieves 2.13\% better accuracy then axis-aligned under the same parameters. You can see \cref{fig:q2-fig7}, indicating that the two-pixel test has higher information gain per split. Other methods, such as linear and non-linear weak learners, were also tested but omitted from the report due to impractically long training and testing times.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{image/q2-fig6.png}
	\caption{Axis-aligned vs Two-pixel test}
	\label{fig:q2-fig6}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{image/q2-fig7-1.png}
	\caption{Axis-aligned vs Two-pixel test: Train accuracy according to depth of tree}
	\label{fig:q2-fig7}
\end{figure}

The main results' confusion matrix, example success/failures, example node's histogram representing information gain along splitting are written in \cref{subsec:Q2-app1} and \cref{subsec:Q2-app2}